EFPA
Some weights of LongformerModel were not initialized from the model checkpoint at tororoin/longformer-8bitadam-2048-main and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Input ids are automatically padded from 26 to 512 to be a multiple of `config.attention_window`: 512
operating on device: cuda:0

batch 1/1260 ----- loss:      1.683 ----- accuracy:      0.344 ----- f1_score:      0.311 ----- precision:      0.543 ----- recall:      0.650
batch 17/1260 ----- loss:      1.594 ----- accuracy:      0.578 ----- f1_score:      0.485 ----- precision:      0.517 ----- recall:      0.530
batch 33/1260 ----- loss:      1.169 ----- accuracy:      0.811 ----- f1_score:      0.564 ----- precision:      0.570 ----- recall:      0.560
batch 49/1260 ----- loss:      1.010 ----- accuracy:      0.893 ----- f1_score:      0.505 ----- precision:      0.593 ----- recall:      0.514
batch 65/1260 ----- loss:      0.948 ----- accuracy:      0.832 ----- f1_score:      0.465 ----- precision:      0.916 ----- recall:      0.506
batch 81/1260 ----- loss:      0.901 ----- accuracy:      0.752 ----- f1_score:      0.437 ----- precision:      0.544 ----- recall:      0.501
batch 97/1260 ----- loss:      0.786 ----- accuracy:      0.895 ----- f1_score:      0.472 ----- precision:      0.447 ----- recall:      0.500
batch 113/1260 ----- loss:      0.782 ----- accuracy:      0.883 ----- f1_score:      0.469 ----- precision:      0.441 ----- recall:      0.500
batch 129/1260 ----- loss:      0.673 ----- accuracy:      0.906 ----- f1_score:      0.475 ----- precision:      0.453 ----- recall:      0.500
batch 145/1260 ----- loss:      0.612 ----- accuracy:      0.939 ----- f1_score:      0.484 ----- precision:      0.470 ----- recall:      0.500
batch 161/1260 ----- loss:      0.591 ----- accuracy:      0.916 ----- f1_score:      0.478 ----- precision:      0.458 ----- recall:      0.500
batch 177/1260 ----- loss:      1.071 ----- accuracy:      0.799 ----- f1_score:      0.444 ----- precision:      0.399 ----- recall:      0.500
batch 193/1260 ----- loss:      0.511 ----- accuracy:      0.795 ----- f1_score:      0.443 ----- precision:      0.397 ----- recall:      0.500
batch 209/1260 ----- loss:      0.573 ----- accuracy:      0.908 ----- f1_score:      0.476 ----- precision:      0.455 ----- recall:      0.499
batch 225/1260 ----- loss:      0.536 ----- accuracy:      0.895 ----- f1_score:      0.472 ----- precision:      0.447 ----- recall:      0.500
batch 241/1260 ----- loss:      0.437 ----- accuracy:      0.910 ----- f1_score:      0.476 ----- precision:      0.455 ----- recall:      0.500
batch 257/1260 ----- loss:      0.540 ----- accuracy:      0.865 ----- f1_score:      0.464 ----- precision:      0.433 ----- recall:      0.500
batch 273/1260 ----- loss:      1.105 ----- accuracy:      0.916 ----- f1_score:      0.478 ----- precision:      0.458 ----- recall:      0.500
batch 289/1260 ----- loss:      0.386 ----- accuracy:      0.799 ----- f1_score:      0.444 ----- precision:      0.399 ----- recall:      0.500
batch 305/1260 ----- loss:      0.424 ----- accuracy:      0.859 ----- f1_score:      0.462 ----- precision:      0.430 ----- recall:      0.500
batch 321/1260 ----- loss:      0.553 ----- accuracy:      0.900 ----- f1_score:      0.474 ----- precision:      0.450 ----- recall:      0.500
batch 337/1260 ----- loss:      1.088 ----- accuracy:      0.852 ----- f1_score:      0.460 ----- precision:      0.426 ----- recall:      0.500
batch 353/1260 ----- loss:      0.605 ----- accuracy:      0.867 ----- f1_score:      0.464 ----- precision:      0.434 ----- recall:      0.500
batch 369/1260 ----- loss:      0.771 ----- accuracy:      0.832 ----- f1_score:      0.454 ----- precision:      0.416 ----- recall:      0.500
batch 385/1260 ----- loss:      0.349 ----- accuracy:      0.869 ----- f1_score:      0.465 ----- precision:      0.435 ----- recall:      0.500
batch 401/1260 ----- loss:      0.378 ----- accuracy:      0.846 ----- f1_score:      0.458 ----- precision:      0.423 ----- recall:      0.500
batch 417/1260 ----- loss:      1.352 ----- accuracy:      0.812 ----- f1_score:      0.448 ----- precision:      0.406 ----- recall:      0.500
batch 433/1260 ----- loss:      0.371 ----- accuracy:      0.889 ----- f1_score:      0.471 ----- precision:      0.444 ----- recall:      0.500
batch 449/1260 ----- loss:      0.421 ----- accuracy:      0.844 ----- f1_score:      0.458 ----- precision:      0.422 ----- recall:      0.500
batch 465/1260 ----- loss:      0.589 ----- accuracy:      0.863 ----- f1_score:      0.463 ----- precision:      0.432 ----- recall:      0.500
batch 481/1260 ----- loss:      0.658 ----- accuracy:      0.848 ----- f1_score:      0.459 ----- precision:      0.424 ----- recall:      0.500
batch 497/1260 ----- loss:      0.467 ----- accuracy:      0.836 ----- f1_score:      0.455 ----- precision:      0.418 ----- recall:      0.500
batch 513/1260 ----- loss:      0.637 ----- accuracy:      0.848 ----- f1_score:      0.459 ----- precision:      0.424 ----- recall:      0.500
batch 529/1260 ----- loss:      0.302 ----- accuracy:      0.908 ----- f1_score:      0.476 ----- precision:      0.454 ----- recall:      0.500
batch 545/1260 ----- loss:      0.388 ----- accuracy:      0.887 ----- f1_score:      0.470 ----- precision:      0.443 ----- recall:      0.500
batch 561/1260 ----- loss:      0.502 ----- accuracy:      0.869 ----- f1_score:      0.465 ----- precision:      0.435 ----- recall:      0.500
batch 577/1260 ----- loss:      0.355 ----- accuracy:      0.873 ----- f1_score:      0.466 ----- precision:      0.437 ----- recall:      0.500
batch 593/1260 ----- loss:      1.347 ----- accuracy:      0.822 ----- f1_score:      0.451 ----- precision:      0.411 ----- recall:      0.500
batch 609/1260 ----- loss:      0.455 ----- accuracy:      0.855 ----- f1_score:      0.461 ----- precision:      0.428 ----- recall:      0.500
batch 625/1260 ----- loss:      0.518 ----- accuracy:      0.887 ----- f1_score:      0.470 ----- precision:      0.443 ----- recall:      0.500
batch 641/1260 ----- loss:      0.326 ----- accuracy:      0.910 ----- f1_score:      0.476 ----- precision:      0.455 ----- recall:      0.500
batch 657/1260 ----- loss:      0.314 ----- accuracy:      0.852 ----- f1_score:      0.460 ----- precision:      0.426 ----- recall:      0.500
batch 673/1260 ----- loss:      0.308 ----- accuracy:      0.852 ----- f1_score:      0.460 ----- precision:      0.426 ----- recall:      0.500
batch 689/1260 ----- loss:      0.383 ----- accuracy:      0.869 ----- f1_score:      0.465 ----- precision:      0.435 ----- recall:      0.500
batch 705/1260 ----- loss:      1.013 ----- accuracy:      0.840 ----- f1_score:      0.456 ----- precision:      0.420 ----- recall:      0.500
batch 721/1260 ----- loss:      0.317 ----- accuracy:      0.844 ----- f1_score:      0.458 ----- precision:      0.422 ----- recall:      0.500
batch 737/1260 ----- loss:      0.850 ----- accuracy:      0.850 ----- f1_score:      0.459 ----- precision:      0.425 ----- recall:      0.500
batch 753/1260 ----- loss:      0.598 ----- accuracy:      0.867 ----- f1_score:      0.464 ----- precision:      0.434 ----- recall:      0.500
batch 769/1260 ----- loss:      0.443 ----- accuracy:      0.848 ----- f1_score:      0.459 ----- precision:      0.424 ----- recall:      0.500
batch 785/1260 ----- loss:      0.310 ----- accuracy:      0.840 ----- f1_score:      0.456 ----- precision:      0.420 ----- recall:      0.500
batch 801/1260 ----- loss:      0.998 ----- accuracy:      0.764 ----- f1_score:      0.433 ----- precision:      0.382 ----- recall:      0.500
batch 817/1260 ----- loss:      1.030 ----- accuracy:      0.830 ----- f1_score:      0.454 ----- precision:      0.416 ----- recall:      0.499
batch 833/1260 ----- loss:      0.388 ----- accuracy:      0.820 ----- f1_score:      0.451 ----- precision:      0.410 ----- recall:      0.500
batch 849/1260 ----- loss:      0.564 ----- accuracy:      0.814 ----- f1_score:      0.449 ----- precision:      0.407 ----- recall:      0.500
batch 865/1260 ----- loss:      0.393 ----- accuracy:      0.828 ----- f1_score:      0.453 ----- precision:      0.414 ----- recall:      0.500
batch 881/1260 ----- loss:      0.367 ----- accuracy:      0.869 ----- f1_score:      0.465 ----- precision:      0.435 ----- recall:      0.500
batch 897/1260 ----- loss:      0.328 ----- accuracy:      0.881 ----- f1_score:      0.468 ----- precision:      0.440 ----- recall:      0.500
batch 913/1260 ----- loss:      0.746 ----- accuracy:      0.773 ----- f1_score:      0.436 ----- precision:      0.387 ----- recall:      0.500
batch 929/1260 ----- loss:      0.916 ----- accuracy:      0.834 ----- f1_score:      0.455 ----- precision:      0.417 ----- recall:      0.500
batch 945/1260 ----- loss:      1.347 ----- accuracy:      0.834 ----- f1_score:      0.455 ----- precision:      0.417 ----- recall:      0.500
batch 961/1260 ----- loss:      0.310 ----- accuracy:      0.826 ----- f1_score:      0.452 ----- precision:      0.413 ----- recall:      0.500
batch 977/1260 ----- loss:      0.360 ----- accuracy:      0.844 ----- f1_score:      0.458 ----- precision:      0.422 ----- recall:      0.500
batch 993/1260 ----- loss:      0.587 ----- accuracy:      0.873 ----- f1_score:      0.466 ----- precision:      0.437 ----- recall:      0.500
batch 1009/1260 ----- loss:      1.258 ----- accuracy:      0.877 ----- f1_score:      0.467 ----- precision:      0.438 ----- recall:      0.500
batch 1025/1260 ----- loss:      0.357 ----- accuracy:      0.842 ----- f1_score:      0.457 ----- precision:      0.421 ----- recall:      0.500
batch 1041/1260 ----- loss:      0.360 ----- accuracy:      0.854 ----- f1_score:      0.460 ----- precision:      0.427 ----- recall:      0.500
batch 1057/1260 ----- loss:      0.882 ----- accuracy:      0.828 ----- f1_score:      0.453 ----- precision:      0.414 ----- recall:      0.500
batch 1073/1260 ----- loss:      0.827 ----- accuracy:      0.863 ----- f1_score:      0.463 ----- precision:      0.432 ----- recall:      0.500
batch 1089/1260 ----- loss:      0.728 ----- accuracy:      0.838 ----- f1_score:      0.456 ----- precision:      0.419 ----- recall:      0.500
batch 1105/1260 ----- loss:      1.011 ----- accuracy:      0.857 ----- f1_score:      0.462 ----- precision:      0.429 ----- recall:      0.500
batch 1121/1260 ----- loss:      1.023 ----- accuracy:      0.875 ----- f1_score:      0.467 ----- precision:      0.438 ----- recall:      0.500
batch 1137/1260 ----- loss:      0.277 ----- accuracy:      0.898 ----- f1_score:      0.473 ----- precision:      0.449 ----- recall:      0.500
batch 1153/1260 ----- loss:      0.550 ----- accuracy:      0.871 ----- f1_score:      0.466 ----- precision:      0.436 ----- recall:      0.500
batch 1169/1260 ----- loss:      0.362 ----- accuracy:      0.885 ----- f1_score:      0.469 ----- precision:      0.442 ----- recall:      0.500
batch 1185/1260 ----- loss:      0.758 ----- accuracy:      0.896 ----- f1_score:      0.473 ----- precision:      0.448 ----- recall:      0.500
batch 1201/1260 ----- loss:      1.393 ----- accuracy:      0.854 ----- f1_score:      0.460 ----- precision:      0.427 ----- recall:      0.500
batch 1217/1260 ----- loss:      0.451 ----- accuracy:      0.795 ----- f1_score:      0.443 ----- precision:      0.397 ----- recall:      0.500
batch 1233/1260 ----- loss:      0.350 ----- accuracy:      0.859 ----- f1_score:      0.462 ----- precision:      0.430 ----- recall:      0.500
batch 1249/1260 ----- loss:      0.668 ----- accuracy:      0.850 ----- f1_score:      0.459 ----- precision:      0.425 ----- recall:      0.500
batch 1260/1260 ----- loss:      0.513 ----- accuracy:      0.881 ----- f1_score:      0.468 ----- precision:      0.440 ----- recall:      0.500
                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
EPOCH 1 training loss:      0.590 - validation loss:      0.569
EPOCH 1 training accuracy:      0.856 - validation accuracy:      0.866
EPOCH 1 training f1_score:      0.507 - validation f1_score:      0.514
EPOCH 1 training precision:      0.550 - validation precision:      0.553
EPOCH 1 training recall:      0.478 - validation recall:      0.486
----------------------------------------------------------------------------------------------------

train set: 
        false positive: 5788 false negative: 0 true positive: 34532 true negative: 0. 
        Just timeouts: 2520 total element to discard: 34532.0 undetected timeouts: 30170 true timeouts: 30170
        
        oracles:
        good oracle: 165,640.67 bad oracle: 90,469,656.37 random oracle: 64,790,986.70 order: 244,178.78
        virtual best: 168,468.87 single best: 246,239.13 
        good oracle/vb: 0.98 good oracle/sb: 0.67 
        bad oracle/vb: 537.01 bad oracle/sb: 367.41 
        random oracle/vb: 384.59 random oracle/sb: 263.12 
        order/vb: 1.45 order/sb: 0.99 
        
validation set: 
        false positive: 606 false negative: 0 true positive: 3906 true negative: 0. 
        Just timeouts: 282 total element to discard: 3906.0 undetected timeouts: 3400 true timeouts: 3400
        
        oracles:
        good oracle: 14,579.39 bad oracle: 10,119,604.29 random oracle: 7,269,783.77 order: 28,748.47
        virtual best: 18,071.17 single best: 29,001.28 
        good oracle/vb: 0.81 good oracle/sb: 0.5 
        bad oracle/vb: 559.99 bad oracle/sb: 348.94 
        random oracle/vb: 402.29 random oracle/sb: 250.67 
        order/vb: 1.59 order/sb: 0.99 
        
test set: 
        false positive: 609 false negative: 0 true positive: 4367 true negative: 0. 
        Just timeouts: 311 total element to discard: 4367.0 undetected timeouts: 3818 true timeouts: 3818
        
        oracles:
        good oracle: 25,186.49 bad oracle: 11,196,000.00 random oracle: 8,091,083.89 order: 35,661.13
        virtual best: 18,866.51 single best: 33,347.97 
        good oracle/vb: 1.33 good oracle/sb: 0.76 
        bad oracle/vb: 66.46 bad oracle/sb: 335.73 
        random oracle/vb: 428.86 random oracle/sb: 242.63 
        order/vb: 1.89 order/sb: 1.07 
        
Elapsed time: 328 seconds
