AbnormalMagicHexagons
Some weights of LongformerModel were not initialized from the model checkpoint at tororoin/longformer-8bitadam-2048-main and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Input ids are automatically padded from 13 to 512 to be a multiple of `config.attention_window`: 512
operating on device: cuda:0

batch 1/243 ----- loss:      0.982 ----- accuracy:      0.812 ----- f1_score:      0.448 ----- precision:      0.500 ----- recall:      0.406
batch 9/243 ----- loss:      0.852 ----- accuracy:      0.836 ----- f1_score:      0.455 ----- precision:      0.495 ----- recall:      0.421
batch 17/243 ----- loss:      0.758 ----- accuracy:      0.984 ----- f1_score:      0.496 ----- precision:      0.496 ----- recall:      0.496
batch 25/243 ----- loss:      0.673 ----- accuracy:      0.992 ----- f1_score:      0.498 ----- precision:      0.496 ----- recall:      0.500
batch 33/243 ----- loss:      0.525 ----- accuracy:      0.984 ----- f1_score:      0.496 ----- precision:      0.492 ----- recall:      0.500
batch 41/243 ----- loss:      0.518 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 49/243 ----- loss:      0.480 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 57/243 ----- loss:      0.412 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 65/243 ----- loss:      0.374 ----- accuracy:      0.969 ----- f1_score:      0.492 ----- precision:      0.484 ----- recall:      0.500
batch 73/243 ----- loss:      0.376 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 81/243 ----- loss:      0.343 ----- accuracy:      0.984 ----- f1_score:      0.496 ----- precision:      0.492 ----- recall:      0.500
batch 89/243 ----- loss:      0.311 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 97/243 ----- loss:      0.285 ----- accuracy:      0.977 ----- f1_score:      0.494 ----- precision:      0.488 ----- recall:      0.500
batch 105/243 ----- loss:      0.254 ----- accuracy:      0.992 ----- f1_score:      0.498 ----- precision:      0.496 ----- recall:      0.500
batch 113/243 ----- loss:      0.275 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 121/243 ----- loss:      0.382 ----- accuracy:      0.977 ----- f1_score:      0.494 ----- precision:      0.488 ----- recall:      0.500
batch 129/243 ----- loss:      0.236 ----- accuracy:      0.984 ----- f1_score:      0.496 ----- precision:      0.492 ----- recall:      0.500
batch 137/243 ----- loss:      0.211 ----- accuracy:      0.984 ----- f1_score:      0.496 ----- precision:      0.492 ----- recall:      0.500
batch 145/243 ----- loss:      0.234 ----- accuracy:      0.977 ----- f1_score:      0.494 ----- precision:      0.488 ----- recall:      0.500
batch 153/243 ----- loss:      0.204 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 161/243 ----- loss:      0.386 ----- accuracy:      0.992 ----- f1_score:      0.498 ----- precision:      0.496 ----- recall:      0.500
batch 169/243 ----- loss:      0.200 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 177/243 ----- loss:      0.172 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 185/243 ----- loss:      0.190 ----- accuracy:      0.984 ----- f1_score:      0.496 ----- precision:      0.492 ----- recall:      0.500
batch 193/243 ----- loss:      0.156 ----- accuracy:      0.992 ----- f1_score:      0.498 ----- precision:      0.496 ----- recall:      0.500
batch 201/243 ----- loss:      0.892 ----- accuracy:      0.953 ----- f1_score:      0.488 ----- precision:      0.477 ----- recall:      0.500
batch 209/243 ----- loss:      0.149 ----- accuracy:      0.984 ----- f1_score:      0.496 ----- precision:      0.492 ----- recall:      0.500
batch 217/243 ----- loss:      0.152 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 225/243 ----- loss:      0.144 ----- accuracy:      0.969 ----- f1_score:      0.492 ----- precision:      0.484 ----- recall:      0.500
batch 233/243 ----- loss:      0.145 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 241/243 ----- loss:      0.132 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
batch 243/243 ----- loss:      0.124 ----- accuracy:      1.000 ----- f1_score:      1.000 ----- precision:      1.000 ----- recall:      1.000
                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
EPOCH 1 training loss:      0.175 - validation loss:      0.137
EPOCH 1 training accuracy:      0.989 - validation accuracy:      0.996
EPOCH 1 training f1_score:      0.935 - validation f1_score:      0.964
EPOCH 1 training precision:      0.938 - validation precision:      0.966
EPOCH 1 training recall:      0.933 - validation recall:      0.963
----------------------------------------------------------------------------------------------------

train set: 
        false positive: 0 false negative: 41 true positive: 0 true negative: 3847. 
        Just timeouts: 0 total element to discard: 41.0 undetected timeouts: 41 true timeouts: 41
        
        oracles:
        good oracle: 488.99 bad oracle: 1,099,666.94 random oracle: 189,240.30 order: 712.63
        virtual best: 486.52 single best: 710.56 
        good oracle/vb: 1.01 good oracle/sb: 0.69 
        bad oracle/vb: 2260.27 bad oracle/sb: 1547.61 
        random oracle/vb: 388.97 random oracle/sb: 266.33 
        order/vb: 1.46 order/sb: 1.0 
        
validation set: 
        false positive: 0 false negative: 2 true positive: 0 true negative: 454. 
        Just timeouts: 0 total element to discard: 2.0 undetected timeouts: 2 true timeouts: 2
        
        oracles:
        good oracle: 48.31 bad oracle: 72,078.60 random oracle: 36,068.26 order: 74.63
        virtual best: 57.92 single best: 82.23 
        good oracle/vb: 0.83 good oracle/sb: 0.59 
        bad oracle/vb: 1244.45 bad oracle/sb: 876.55 
        random oracle/vb: 622.73 random oracle/sb: 438.63 
        order/vb: 1.29 order/sb: 0.91 
        
test set: 
        false positive: 0 false negative: 5 true positive: 0 true negative: 475. 
        Just timeouts: 0 total element to discard: 5.0 undetected timeouts: 5 true timeouts: 5
        
        oracles:
        good oracle: 58.45 bad oracle: 144,066.03 random oracle: 79,197.68 order: 84.30
        virtual best: 51.31 single best: 78.77 
        good oracle/vb: 1.14 good oracle/sb: 0.74 
        bad oracle/vb: 296.12 bad oracle/sb: 1828.95 
        random oracle/vb: 1543.51 random oracle/sb: 1005.43 
        order/vb: 1.64 order/sb: 1.07 
        
Elapsed time: 115 seconds
